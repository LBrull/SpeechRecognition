{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(Sergio)CÃ²pia de Proyecto grupo.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rOz369_S5TeA"},"source":["## Install & Import"]},{"cell_type":"markdown","metadata":{"id":"mWKhoMIlz6ml"},"source":["###Define path"]},{"cell_type":"code","metadata":{"id":"sxq2SYcrLA0M","executionInfo":{"status":"ok","timestamp":1605618312705,"user_tz":-60,"elapsed":1922,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"cdc28d21-de34-4cff-969c-a1213adcbbcb","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive' , force_remount=True)\n","path = '/content/drive/My Drive/Project AI'\n","path_txt = '/flickr_text/'\n","audio_path = '/flickr_audio/wavs/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LLBgqneV1BWl","executionInfo":{"status":"ok","timestamp":1605618312708,"user_tz":-60,"elapsed":1916,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"ee1cb93d-1c14-48ea-f55d-1a6f74b0d81e","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd /content/drive/My Drive/Project AI/pase-master"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Project AI/pase-master\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"88jDLwchA0gQ"},"source":["###Install"]},{"cell_type":"code","metadata":{"id":"Z4rX0Yi9Azip","executionInfo":{"status":"ok","timestamp":1605618314926,"user_tz":-60,"elapsed":4113,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"e61629cc-72e2-454a-c5eb-7e33616866c2","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install more-itertools"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (8.6.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tx7lASdwIJkS","executionInfo":{"status":"ok","timestamp":1605618337337,"user_tz":-60,"elapsed":26514,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"0a39ec8d-0815-4c9a-a42d-accd8cca239a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install --upgrade -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/santi-pdp/ahoproc_tools.git (from -r requirements.txt (line 2))\n","  Cloning https://github.com/santi-pdp/ahoproc_tools.git to /tmp/pip-req-build-x8zgiyhs\n","  Running command git clone -q https://github.com/santi-pdp/ahoproc_tools.git /tmp/pip-req-build-x8zgiyhs\n","Collecting git+https://github.com/salesforce/pytorch-qrnn (from -r requirements.txt (line 16))\n","  Cloning https://github.com/salesforce/pytorch-qrnn to /tmp/pip-req-build-l77s71k4\n","  Running command git clone -q https://github.com/salesforce/pytorch-qrnn /tmp/pip-req-build-l77s71k4\n","Collecting git+https://github.com/detly/gammatone (from -r requirements.txt (line 17))\n","  Cloning https://github.com/detly/gammatone to /tmp/pip-req-build-91xcgo3c\n","  Running command git clone -q https://github.com/detly/gammatone /tmp/pip-req-build-91xcgo3c\n","Collecting git+https://github.com/pswietojanski/kaldi-io-for-python.git (from -r requirements.txt (line 18))\n","  Cloning https://github.com/pswietojanski/kaldi-io-for-python.git to /tmp/pip-req-build-mhx1ynyn\n","  Running command git clone -q https://github.com/pswietojanski/kaldi-io-for-python.git /tmp/pip-req-build-mhx1ynyn\n","Requirement already up-to-date: tqdm==4.28.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (4.28.1)\n","Requirement already up-to-date: numpy==1.16.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.16.4)\n","Requirement already up-to-date: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.1.0)\n","Requirement already up-to-date: librosa==0.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.6.3)\n","Requirement already up-to-date: SoundFile==0.10.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.10.2)\n","Requirement already up-to-date: torchaudio==0.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.4.0)\n","Requirement already up-to-date: pysptk==0.1.16 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.1.16)\n","Requirement already up-to-date: matplotlib==3.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (3.0.2)\n","Requirement already up-to-date: python_speech_features==0.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.6)\n","Requirement already up-to-date: scikit_learn==0.20.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.20.3)\n","Requirement already up-to-date: tensorboardX==1.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (1.6)\n","Requirement already up-to-date: webrtcvad==2.0.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (2.0.10)\n","Requirement already up-to-date: cupy-cuda101 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (8.1.0)\n","Requirement already up-to-date: pynvrtc==8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (8.0)\n","Requirement already satisfied, skipping upgrade: sklearn in /usr/local/lib/python3.6/dist-packages (from ahoproc-tools==1.0.0->-r requirements.txt (line 2)) (0.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from ahoproc-tools==1.0.0->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied, skipping upgrade: nose in /usr/local/lib/python3.6/dist-packages (from Gammatone==1.0->-r requirements.txt (line 17)) (1.3.7)\n","Requirement already satisfied, skipping upgrade: mock in /usr/local/lib/python3.6/dist-packages (from Gammatone==1.0->-r requirements.txt (line 17)) (4.0.2)\n","Requirement already satisfied, skipping upgrade: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 5)) (0.2.2)\n","Requirement already satisfied, skipping upgrade: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 5)) (0.17.0)\n","Requirement already satisfied, skipping upgrade: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 5)) (0.48.0)\n","Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 5)) (2.1.9)\n","Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.6.3->-r requirements.txt (line 5)) (4.4.2)\n","Requirement already satisfied, skipping upgrade: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from SoundFile==0.10.2->-r requirements.txt (line 6)) (1.14.3)\n","Requirement already satisfied, skipping upgrade: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchaudio==0.4.0->-r requirements.txt (line 7)) (1.4.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.2->-r requirements.txt (line 9)) (1.3.1)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.2->-r requirements.txt (line 9)) (0.10.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.2->-r requirements.txt (line 9)) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.2->-r requirements.txt (line 9)) (2.4.7)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.6->-r requirements.txt (line 12)) (3.12.4)\n","Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda101->-r requirements.txt (line 14)) (0.5)\n","Requirement already satisfied, skipping upgrade: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa==0.6.3->-r requirements.txt (line 5)) (0.31.0)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa==0.6.3->-r requirements.txt (line 5)) (50.3.2)\n","Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->SoundFile==0.10.2->-r requirements.txt (line 6)) (2.20)\n","Building wheels for collected packages: ahoproc-tools, PyTorch-QRNN, Gammatone, kaldi-io\n","  Building wheel for ahoproc-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ahoproc-tools: filename=ahoproc_tools-1.0.0-py2.py3-none-any.whl size=7860 sha256=c54ceee750095d32d7f2314e1bd311b78d871fe6ee8fcf7cc46a4aeaf038b389\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jtvm9688/wheels/67/b2/cb/aa62b80d36765b9704d10a3f51c040d13be95aa4f9968b7870\n","  Building wheel for PyTorch-QRNN (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyTorch-QRNN: filename=PyTorch_QRNN-0.2.1-cp36-none-any.whl size=8264 sha256=ca2b9f1ddf81f1c207e234ff3023c9c5558a8fac6e867613c0f61d2af1e3a7f3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jtvm9688/wheels/75/d0/b7/828c11a6f168f188783c7cebdc152bd72c629d6b8eabe55ded\n","  Building wheel for Gammatone (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Gammatone: filename=Gammatone-1.0-cp36-none-any.whl size=21794 sha256=7ae6d47d4a7769da2368adac9f41c752a408a74a8aebe0afd324688d16e89ae5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jtvm9688/wheels/5e/7e/7e/a0ec0f027829761aff44281cc9cf9992168f82712af64b87ba\n","  Building wheel for kaldi-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaldi-io: filename=kaldi_io-0.9.0-cp36-none-any.whl size=13802 sha256=ae8badd6a606b62b3509dd75b9ad69348262d8ec16debd9f84d583975ae75a01\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jtvm9688/wheels/3d/90/6f/d450c9ecb01843d63b93d36c16da03863f8cfefec185dbed6e\n","Successfully built ahoproc-tools PyTorch-QRNN Gammatone kaldi-io\n","Installing collected packages: ahoproc-tools, PyTorch-QRNN, Gammatone, kaldi-io\n","  Found existing installation: ahoproc-tools 1.0.0\n","    Uninstalling ahoproc-tools-1.0.0:\n","      Successfully uninstalled ahoproc-tools-1.0.0\n","  Found existing installation: PyTorch-QRNN 0.2.1\n","    Uninstalling PyTorch-QRNN-0.2.1:\n","      Successfully uninstalled PyTorch-QRNN-0.2.1\n","  Found existing installation: Gammatone 1.0\n","    Uninstalling Gammatone-1.0:\n","      Successfully uninstalled Gammatone-1.0\n","  Found existing installation: kaldi-io 0.9.0\n","    Uninstalling kaldi-io-0.9.0:\n","      Successfully uninstalled kaldi-io-0.9.0\n","Successfully installed Gammatone-1.0 PyTorch-QRNN-0.2.1 ahoproc-tools-1.0.0 kaldi-io-0.9.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["kaldi_io","torchqrnn"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"RNPK8LSzIN5n","executionInfo":{"status":"ok","timestamp":1605618339441,"user_tz":-60,"elapsed":28609,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"a4336706-4cf1-4506-bdc1-b84885376344","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install torch==1.4.0 torchvision==0.5.0 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0+cu92)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.16.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lK0zrBGlA9aa","executionInfo":{"status":"ok","timestamp":1605618341862,"user_tz":-60,"elapsed":31022,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"2dbc1dfe-3cb5-408b-dbfd-a8685a4d931b","colab":{"base_uri":"https://localhost:8080/"}},"source":["!python setup.py install"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/setuptools/dist.py:452: UserWarning: Normalizing '0.1.1-dev' to '0.1.1.dev0'\n","  warnings.warn(tmpl.format(**locals()))\n","running install\n","running bdist_egg\n","running egg_info\n","writing PASE.egg-info/PKG-INFO\n","writing dependency_links to PASE.egg-info/dependency_links.txt\n","writing top-level names to PASE.egg-info/top_level.txt\n","reading manifest file 'PASE.egg-info/SOURCES.txt'\n","writing manifest file 'PASE.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/pase\n","copying build/lib/pase/utils.py -> build/bdist.linux-x86_64/egg/pase\n","copying build/lib/pase/sbatch_writer.py -> build/bdist.linux-x86_64/egg/pase\n","copying build/lib/pase/__init__.py -> build/bdist.linux-x86_64/egg/pase\n","copying build/lib/pase/dataset.py -> build/bdist.linux-x86_64/egg/pase\n","copying build/lib/pase/log.py -> build/bdist.linux-x86_64/egg/pase\n","copying build/lib/pase/transforms.py -> build/bdist.linux-x86_64/egg/pase\n","copying build/lib/pase/losses.py -> build/bdist.linux-x86_64/egg/pase\n","creating build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/encoders.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/__init__.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/classifiers.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/modules.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/pase.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/decoders.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/discriminator.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/frontend.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/core.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/aspp.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/tdnn.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/neural_networks.py -> build/bdist.linux-x86_64/egg/pase/models\n","copying build/lib/pase/models/attention_block.py -> build/bdist.linux-x86_64/egg/pase/models\n","creating build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","copying build/lib/pase/models/WorkerScheduler/worker_scheduler.py -> build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","copying build/lib/pase/models/WorkerScheduler/trainer.py -> build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","copying build/lib/pase/models/WorkerScheduler/min_norm_solvers.py -> build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","copying build/lib/pase/models/WorkerScheduler/radam.py -> build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","copying build/lib/pase/models/WorkerScheduler/lr_scheduler.py -> build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","copying build/lib/pase/models/WorkerScheduler/__init__.py -> build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","copying build/lib/pase/models/WorkerScheduler/encoder.py -> build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler\n","creating build/bdist.linux-x86_64/egg/pase/models/Minions\n","copying build/lib/pase/models/Minions/minions.py -> build/bdist.linux-x86_64/egg/pase/models/Minions\n","copying build/lib/pase/models/Minions/cls_minions.py -> build/bdist.linux-x86_64/egg/pase/models/Minions\n","copying build/lib/pase/models/Minions/__init__.py -> build/bdist.linux-x86_64/egg/pase/models/Minions\n","byte-compiling build/bdist.linux-x86_64/egg/pase/utils.py to utils.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/sbatch_writer.py to sbatch_writer.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/dataset.py to dataset.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/log.py to log.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/transforms.py to transforms.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/losses.py to losses.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/encoders.py to encoders.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/classifiers.py to classifiers.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/modules.py to modules.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/pase.py to pase.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/decoders.py to decoders.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/discriminator.py to discriminator.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/frontend.py to frontend.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/core.py to core.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/aspp.py to aspp.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/tdnn.py to tdnn.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/neural_networks.py to neural_networks.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/attention_block.py to attention_block.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler/worker_scheduler.py to worker_scheduler.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler/trainer.py to trainer.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler/min_norm_solvers.py to min_norm_solvers.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler/radam.py to radam.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler/lr_scheduler.py to lr_scheduler.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler/__init__.py to __init__.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/WorkerScheduler/encoder.py to encoder.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/Minions/minions.py to minions.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/Minions/cls_minions.py to cls_minions.cpython-36.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/pase/models/Minions/__init__.py to __init__.cpython-36.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying PASE.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying PASE.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying PASE.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying PASE.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating 'dist/PASE-0.1.1.dev0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing PASE-0.1.1.dev0-py3.6.egg\n","Removing /usr/local/lib/python3.6/dist-packages/PASE-0.1.1.dev0-py3.6.egg\n","Copying PASE-0.1.1.dev0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n","PASE 0.1.1.dev0 is already the active version in easy-install.pth\n","\n","Installed /usr/local/lib/python3.6/dist-packages/PASE-0.1.1.dev0-py3.6.egg\n","Processing dependencies for PASE==0.1.1.dev0\n","Finished processing dependencies for PASE==0.1.1.dev0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JRnbgf7dUjaF"},"source":["### Import"]},{"cell_type":"code","metadata":{"id":"-fHFOe1Q54ij"},"source":["import torch\n","import torchaudio\n","from more_itertools import chunked\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import math, copy, time, cupy\n","import matplotlib.pyplot as plt\n","from IPython.core.debugger import set_trace\n","import spacy\n","from torchtext import data, datasets\n","import torch.optim as optim\n","import plotly.graph_objects as go\n","import time\n","import numpy as np\n","import string\n","import pandas as pd\n","import glob\n","from sklearn.utils import shuffle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0q--e_9Xa4v","executionInfo":{"status":"ok","timestamp":1605618341863,"user_tz":-60,"elapsed":31010,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"35c63128-f4ec-4b6a-f3c2-4f80c585cdba","colab":{"base_uri":"https://localhost:8080/"}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"gkDpfh5FBHyK","executionInfo":{"status":"ok","timestamp":1605618345205,"user_tz":-60,"elapsed":34340,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"b4fbcf46-422e-45b5-a9ab-90167d73e25b","colab":{"base_uri":"https://localhost:8080/"}},"source":["from pase.models.frontend import wf_builder\n","pase = wf_builder('cfg/frontend/PASE+.cfg').eval()\n","pase.load_pretrained('FE_e199.ckpt', load_last=True, verbose=True)\n","pase = pase.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING: QRNN ignores bidirectional flag\n","Current Model keys:  78\n","Current Pt keys:  78\n","Loading matching keys:  ['denseskips.0.weight', 'denseskips.1.weight', 'denseskips.2.weight', 'denseskips.3.weight', 'denseskips.4.weight', 'denseskips.5.weight', 'denseskips.6.weight', 'blocks.0.conv.low_hz_', 'blocks.0.conv.band_hz_', 'blocks.0.norm.weight', 'blocks.0.norm.bias', 'blocks.0.norm.running_mean', 'blocks.0.norm.running_var', 'blocks.0.norm.num_batches_tracked', 'blocks.0.act.weight', 'blocks.1.conv.weight', 'blocks.1.conv.bias', 'blocks.1.norm.weight', 'blocks.1.norm.bias', 'blocks.1.norm.running_mean', 'blocks.1.norm.running_var', 'blocks.1.norm.num_batches_tracked', 'blocks.1.act.weight', 'blocks.2.conv.weight', 'blocks.2.conv.bias', 'blocks.2.norm.weight', 'blocks.2.norm.bias', 'blocks.2.norm.running_mean', 'blocks.2.norm.running_var', 'blocks.2.norm.num_batches_tracked', 'blocks.2.act.weight', 'blocks.3.conv.weight', 'blocks.3.conv.bias', 'blocks.3.norm.weight', 'blocks.3.norm.bias', 'blocks.3.norm.running_mean', 'blocks.3.norm.running_var', 'blocks.3.norm.num_batches_tracked', 'blocks.3.act.weight', 'blocks.4.conv.weight', 'blocks.4.conv.bias', 'blocks.4.norm.weight', 'blocks.4.norm.bias', 'blocks.4.norm.running_mean', 'blocks.4.norm.running_var', 'blocks.4.norm.num_batches_tracked', 'blocks.4.act.weight', 'blocks.5.conv.weight', 'blocks.5.conv.bias', 'blocks.5.norm.weight', 'blocks.5.norm.bias', 'blocks.5.norm.running_mean', 'blocks.5.norm.running_var', 'blocks.5.norm.num_batches_tracked', 'blocks.5.act.weight', 'blocks.6.conv.weight', 'blocks.6.conv.bias', 'blocks.6.norm.weight', 'blocks.6.norm.bias', 'blocks.6.norm.running_mean', 'blocks.6.norm.running_var', 'blocks.6.norm.num_batches_tracked', 'blocks.6.act.weight', 'blocks.7.conv.weight', 'blocks.7.conv.bias', 'blocks.7.norm.weight', 'blocks.7.norm.bias', 'blocks.7.norm.running_mean', 'blocks.7.norm.running_var', 'blocks.7.norm.num_batches_tracked', 'blocks.7.act.weight', 'rnn.layers.0.linear.weight', 'rnn.layers.0.linear.bias', 'W.weight', 'W.bias', 'norm_out.running_mean', 'norm_out.running_var', 'norm_out.num_batches_tracked']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lyH2viZCznWH"},"source":["##Get data"]},{"cell_type":"markdown","metadata":{"id":"uLrXXEq20bKc"},"source":["###Create Dataframe for train, dev and test"]},{"cell_type":"code","metadata":{"id":"-7onAdQhzy4T","executionInfo":{"status":"ok","timestamp":1605618345209,"user_tz":-60,"elapsed":34337,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"aa450c6a-ddbb-4169-df58-5f30df71ec66","colab":{"base_uri":"https://localhost:8080/"}},"source":["train_dev_test_split = {}\n","\n","with open (path + path_txt + \"Flickr_8k.devImages.txt\",'r') as dev, open (path + path_txt + \"Flickr_8k.testImages.txt\",'r') as test, open (path + path_txt + \"Flickr_8k.trainImages.txt\",'r') as train:\n","        for line in dev:\n","                train_dev_test_split[line.strip().replace(\".jpg\",\"\")] = \"dev\"\n","        for line in test:\n","                train_dev_test_split[line.strip().replace(\".jpg\",\"\")] = \"test\"\n","        for line in train:\n","                train_dev_test_split[line.strip().replace(\".jpg\",\"\")] = \"train\"\n","                \n","print(len(train_dev_test_split))\n","print(dict(list(train_dev_test_split.items())[0:2]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8000\n","{'2090545563_a4e66ec76b': 'dev', '3393035454_2d2370ffd4': 'dev'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1w_g0Iq-0bam","executionInfo":{"status":"ok","timestamp":1605618345210,"user_tz":-60,"elapsed":34333,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"c4d98c37-2c3c-4534-917a-c55cfd80515d","colab":{"base_uri":"https://localhost:8080/"}},"source":["text_data = pd.read_csv(path + path_txt + 'Flickr8k.lemma.token.txt', header = None,sep='\\t', dtype={'ID': str})\n","text_data.dropna(inplace = True) \n","text_data = text_data.rename(columns={0: \"Image\", 1: \"Description\"})\n","text_data['ID'] = \"\"\n","text_data[['ID','NumAudio']] = text_data.Image.str.split(\"#\",expand=True)\n","text_data['Image'] = text_data['ID']\n","text_data['ID'] = text_data['ID'].str.replace('.jpg','')\n","\n","tdt = []\n","for x in text_data['ID']:\n","    if x in train_dev_test_split.keys():\n","        tdt.append(train_dev_test_split[x])\n","    else:\n","        tdt.append(\"Not_used\")\n","text_data['TrainDevTest'] = tdt\n","\n","text_data['ID'] = text_data['ID'] + '_' + text_data['NumAudio'] + '.wav'\n","text_data = text_data[['ID','Image','Description','TrainDevTest']]\n","\n","print(text_data[text_data.TrainDevTest=='train'].shape[0])\n","print(text_data[text_data.TrainDevTest=='dev'].shape[0])\n","print(text_data[text_data.TrainDevTest=='test'].shape[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["30000\n","5000\n","5000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"diMkmm0v7qz5","executionInfo":{"status":"ok","timestamp":1605618345211,"user_tz":-60,"elapsed":34330,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"785d8c40-e533-4e17-d082-d41ed02f5e94","colab":{"base_uri":"https://localhost:8080/"}},"source":["train_data = text_data[text_data.TrainDevTest=='train'][['ID','Image','Description']][:6000]\n","dev_data = text_data[text_data.TrainDevTest=='dev'][['ID','Image','Description']][:500]\n","test_data = text_data[text_data.TrainDevTest=='test'][['ID','Image','Description']][:500]\n","\n","data_to_model = train_data[['ID','Description']]\n","\n","print(train_data.head())\n","print(data_to_model.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                            ID  ...                                        Description\n","0  1305564994_00513f9a5b_0.wav  ...  A man in street racer armor be examine the tir...\n","1  1305564994_00513f9a5b_1.wav  ...         Two racer drive a white bike down a road .\n","2  1305564994_00513f9a5b_2.wav  ...  Two motorist be ride along on their vehicle th...\n","3  1305564994_00513f9a5b_3.wav  ...  Two person be in a small race car drive by a g...\n","4  1305564994_00513f9a5b_4.wav  ...       Two person in race uniform in a street car .\n","\n","[5 rows x 3 columns]\n","                            ID                                        Description\n","0  1305564994_00513f9a5b_0.wav  A man in street racer armor be examine the tir...\n","1  1305564994_00513f9a5b_1.wav         Two racer drive a white bike down a road .\n","2  1305564994_00513f9a5b_2.wav  Two motorist be ride along on their vehicle th...\n","3  1305564994_00513f9a5b_3.wav  Two person be in a small race car drive by a g...\n","4  1305564994_00513f9a5b_4.wav       Two person in race uniform in a street car .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JLkkd1GdvdkH","executionInfo":{"status":"ok","timestamp":1605618345212,"user_tz":-60,"elapsed":34327,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"b71ca4e2-de8d-4136-c60f-bbdf936a3846","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(len(train_data))\n","print(len(dev_data))\n","print(len(test_data))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6000\n","500\n","500\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oRV1AILWw8mO"},"source":["##Vocabulary generation\n"]},{"cell_type":"markdown","metadata":{"id":"cIDLn4Mjw_PT"},"source":["###Text pre-processing"]},{"cell_type":"code","metadata":{"id":"cJfaStr0xDvy"},"source":["def text_prep(sentence):\n","  return sentence.translate(str.maketrans('', '', string.punctuation)).lower() # remove punctuations, to lower case"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPRDxQq0oCJI"},"source":["### Vocabulary -> character level"]},{"cell_type":"code","metadata":{"id":"_ASYHNhJoAqL","executionInfo":{"status":"ok","timestamp":1605618346014,"user_tz":-60,"elapsed":35121,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"d9f6acc7-003f-4a8f-f588-bb7f9649ac77","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_to_index = {}\n","index_to_word = {}\n","index = 0\n","max_len = 0 #max characters in one sentence\n","\n","with open(path + \"/flickr_text/Flickr8k.lemma.token.txt\",\"r\",encoding=\"utf-8\",errors=\"ignore\") as r:\n","    for line in r:\n","        sentence = text_prep(line.strip().split('\\t')[1]) # sentence pre-processing\n","        chars = list(sentence) # get characters \n","        if len(chars) > max_len:\n","          max_len = len(chars)\n","        for char in chars:\n","            if char not in word_to_index.keys():\n","                word_to_index[char] = index\n","                index_to_word[index] = char\n","                index+=1\n","\n","SOS_TOKEN = \"<s>\"\n","EOS_TOKEN = \"</s>\"\n","PAD_TOKEN = \"<pad>\"\n","\n","word_to_index[SOS_TOKEN] = index # add sos token\n","index_to_word[index] = SOS_TOKEN\n","index+=1\n","\n","word_to_index[EOS_TOKEN] = index # add eos token\n","index_to_word[index] = EOS_TOKEN\n","index+=1\n","\n","word_to_index[PAD_TOKEN] = index # add pad token\n","index_to_word[index] = PAD_TOKEN\n","PAD_INDEX = word_to_index[PAD_TOKEN]\n","\n","vocab_size = len(word_to_index) # get vocabulary size\n","max_len+=2 # add one to max length for sentence end token\n","\n","print(max_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["184\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YR3EotY_N6zo","executionInfo":{"status":"ok","timestamp":1605618346015,"user_tz":-60,"elapsed":35118,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"efb516ad-7d4b-49bb-e3dd-e56989236367","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_to_index"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{' ': 1,\n"," '0': 32,\n"," '1': 31,\n"," '2': 30,\n"," '3': 29,\n"," '4': 28,\n"," '5': 27,\n"," '6': 33,\n"," '7': 36,\n"," '8': 35,\n"," '9': 34,\n"," '</s>': 38,\n"," '<pad>': 39,\n"," '<s>': 37,\n"," 'a': 0,\n"," 'b': 11,\n"," 'c': 9,\n"," 'd': 17,\n"," 'e': 8,\n"," 'f': 14,\n"," 'g': 20,\n"," 'h': 13,\n"," 'i': 4,\n"," 'j': 24,\n"," 'k': 15,\n"," 'l': 19,\n"," 'm': 2,\n"," 'n': 3,\n"," 'o': 10,\n"," 'p': 22,\n"," 'q': 25,\n"," 'r': 7,\n"," 's': 5,\n"," 't': 6,\n"," 'u': 23,\n"," 'v': 18,\n"," 'w': 16,\n"," 'x': 12,\n"," 'y': 21,\n"," 'z': 26}"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"gTdZ7Wbi3tOm"},"source":["## Encoder"]},{"cell_type":"markdown","metadata":{"id":"Rlaomvtj_C7t"},"source":["###Encoder-No attention"]},{"cell_type":"code","metadata":{"id":"0M2CUTlwxyWZ"},"source":["class Encoder_no_attention(nn.Module):\n","\n","    def __init__(self, num_inp_channels, num_out_channels, encoder_final_size, \n","               kernel_size = 7, stride = 3, pool_size = 3, dropout = 0.):\n","        super().__init__()\n","        #self.conv = nn.Conv1d(num_inp_channels, num_out_channels, kernel_size=kernel_size, stride=stride) # puede que no haga falta\n","        #self.relu = nn.ReLU() # puede que no haga falta\n","        #self.pool = nn.AvgPool1d(pool_size) #cambiar por average pooling\n","        #self.rnn = nn.GRU(num_out_channels, encoder_final_size, batch_first=True, dropout=dropout) \n","        self.linear = nn.Linear(256,encoder_final_size)\n","    def forward(self, x):\n","        x = x.permute(0,2,1)\n","        x = torch.nn.functional.avg_pool2d(x, (x.shape[1], 1)).squeeze(1)\n","        x = self.linear(x)\n","        #x = x.permute(0,2,1)\n","        #x_final, _ = self.rnn(x)\n","        #x_final = x_final[:,-1]\n","        return x, x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMfZKpGz9OQo"},"source":["###Encoder-Attention"]},{"cell_type":"code","metadata":{"id":"55cBE1FuB0QY"},"source":["class Encoder_attention(nn.Module):\n","\n","    def __init__(self, num_inp_channels, num_out_channels, encoder_final_size, kernel_size = 7, stride = 3, pool_size = 3):\n","        super().__init__()\n","        self.encoder_final_size = encoder_final_size\n","        self.conv = nn.Conv1d(num_inp_channels, num_out_channels, kernel_size=kernel_size, stride=stride) # puede que no haga falta\n","        self.relu = nn.ReLU() # puede que no haga falta\n","        self.maxpool = nn.AvgPool1d(pool_size) #cambiar por average pooling\n","        self.linear = nn.Linear(256,encoder_final_size)\n","        #self.rnn = nn.GRU(num_out_channels, encoder_final_size, batch_first=True, dropout=dropout) \n","\n","    def forward(self, x):\n","        x = self.maxpool(self.relu(self.conv(x)))\n","        x = x.permute(0,2,1)\n","        encoder_final = torch.nn.functional.avg_pool2d(x, (x.shape[1], 1)).squeeze(1) \n","        encoder_final = self.linear(encoder_final)     \n","        #encoder_final = torch.zeros(x.size(0),self.encoder_final_size) #Use zeros as the encoder final when use attention; can be replaced by image vector\n","\n","        return x, encoder_final"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQC4mqiIzoYQ"},"source":["## Decoder"]},{"cell_type":"markdown","metadata":{"id":"LVUzV_w82Zon"},"source":["###Decoder-No attention"]},{"cell_type":"code","metadata":{"id":"aQU1MrYzpa1x"},"source":["class Decoder_no_attention(nn.Module): \n","   \n","    def __init__(self, emb_size, encoder_final_size, hidden_size, vocab_size, num_layers=1, dropout=0.5):\n","        super(Decoder_no_attention, self).__init__()\n","        \n","        self.emb_size = emb_size # Word Embedding dimension \n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","                 \n","        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n","                 \n","        # Initialization from the final encoder state\n","        self.initialize_hidden = nn.Linear(encoder_final_size, hidden_size, bias=True) # change the size of encoding vector\n","        self.dropout_layer = nn.Dropout(p=dropout)\n","        self.pre_output_layer = nn.Linear(hidden_size + emb_size, hidden_size, bias=False)\n","        \n","    def forward_step(self, prev_embed, hidden):\n","        \n","        rnn_output, hidden = self.rnn(prev_embed, hidden)\n","        pre_output = torch.cat([prev_embed, rnn_output], dim=2)\n","        pre_output = self.dropout_layer(pre_output)\n","        pre_output = self.pre_output_layer(pre_output)\n","\n","        return pre_output, hidden\n","    \n","    def forward(self, trg_embed, encoder_hidden, encoder_final, \n","                max_len, hidden=None):\n","      \n","        if hidden is None:\n","            hidden = self.init_hidden(encoder_final)\n","            \n","\n","        pre_output_vectors = [] # save all output vectors in an array\n","      \n","        for i in range(max_len):\n","            prev_embed = trg_embed[:, i].unsqueeze(1)\n","            pre_output, hidden = self.forward_step(prev_embed, hidden)\n","            pre_output_vectors.append(pre_output)\n","\n","        pre_output_vectors = torch.cat(pre_output_vectors, dim=1) # output shape = (batch, sentence length, vocabulary size)\n","        return pre_output_vectors, hidden\n","\n","    def init_hidden(self, encoder_final):\n","        if encoder_final is None:\n","            print('encoder_final none')\n","            return None\n","        return torch.tanh(self.initialize_hidden(encoder_final).unsqueeze(0).expand(self.num_layers,-1,-1)).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJ2v5AHz2dsA"},"source":["###Decoder-attention"]},{"cell_type":"code","metadata":{"id":"xCjZVbw-Tz4k"},"source":["class AdditiveAttention(nn.Module):    \n","    def __init__(self, hidden_size, key_size=None, query_size=None):\n","        super(AdditiveAttention, self).__init__()\n","        \n","        # initialize key_size and query_size\n","        key_size = hidden_size if key_size is None else key_size\n","        query_size = hidden_size if query_size is None else query_size\n","\n","        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n","        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n","        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n","\n","        self.alphas = None\n","        \n","    def forward(self, key=None, query=None, value=None):\n","        query = self.query_layer(query)\n","\n","        scores = self.energy_layer(torch.tanh(query + key))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","        \n","        alphas = F.softmax(scores, dim=-1)\n","        self.alphas = alphas               \n","        context = torch.bmm(alphas, value)\n","        \n","        return context, alphas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7t9f2ABzU12Z"},"source":["class Decoder_with_attention(nn.Module): \n","   \n","    def __init__(self, attention, emb_size, encoder_final_size, hidden_size, vocab_size, num_layers=2, dropout=0.5):\n","        super(Decoder_with_attention, self).__init__()\n","        \n","        self.emb_size = emb_size # Word Embedding dimension \n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.attention = attention\n","        self.dropout = dropout\n","                 \n","        self.rnn = nn.GRU(emb_size + hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n","                 \n","        # Initialization from the final encoder state\n","        self.initialize_hidden = nn.Linear(encoder_final_size, hidden_size, bias=True) # change the size of encoding vector\n","        self.dropout_layer = nn.Dropout(p=dropout)\n","        self.pre_output_layer = nn.Linear(2*hidden_size + emb_size, hidden_size, bias=False)\n","        self.output_feedforward_layer = nn.Linear(hidden_size, vocab_size, bias=True) # change the size of rnn output to the vocabulary size  \n","        \n","    def forward_step(self, prev_embed, encoder_hidden, key, hidden):\n","        \n","        query = hidden[-1].unsqueeze(1)\n","        context, attn_probs = self.attention(\n","            query=query, key=key,\n","            value=encoder_hidden)\n","\n","        rnn_input = torch.cat([prev_embed, context], dim=2) # concatenate the previous word embedding with the context vector as the rnn input \n","        rnn_output, hidden = self.rnn(rnn_input, hidden)\n","\n","        pre_output = torch.cat([prev_embed, rnn_output, context], dim=2)\n","        pre_output = self.dropout_layer(pre_output)\n","        pre_output = self.pre_output_layer(pre_output)\n","\n","        return pre_output, hidden\n","    \n","    def forward(self, trg_embed, encoder_hidden, encoder_final, \n","                max_len, hidden=None):\n","      \n","        if hidden is None:\n","            hidden = self.init_hidden(encoder_final)\n","\n","        key = self.attention.key_layer(encoder_hidden)\n","\n","        pre_output_vectors = [] # save all output vectors in an array\n","\n","        for i in range(max_len):\n","            prev_embed = trg_embed[:, i].unsqueeze(1)\n","            pre_output, hidden, = self.forward_step(prev_embed, encoder_hidden, key, hidden)\n","            pre_output_vectors.append(pre_output)\n","\n","        pre_output_vectors = torch.cat(pre_output_vectors, dim=1) # output shape = (batch, sentence length, vocabulary size)\n","        return pre_output_vectors, hidden\n","\n","    def init_hidden(self, encoder_final):\n","        if encoder_final is None:\n","            return None\n","        return torch.tanh(self.initialize_hidden(encoder_final).unsqueeze(0).expand(self.num_layers,-1,-1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0lgrNz8z2Sve"},"source":["##Model"]},{"cell_type":"markdown","metadata":{"id":"Dmw57bpJCie2"},"source":["###Generator\n"]},{"cell_type":"code","metadata":{"id":"PNr39tcjCncc"},"source":["class Generator(nn.Module):\n","    def __init__(self, hidden_size, vocab_size):\n","        super(Generator, self).__init__()\n","        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n","\n","    def forward(self, x):\n","        return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mhv40wbi7Aza"},"source":["###Encode-Decoder"]},{"cell_type":"code","metadata":{"id":"WdSZist6QDUD"},"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self, encoder, decoder, embedding, generator):\n","        super(EncoderDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.embedding = embedding\n","        self.generator = generator\n","\n","    def forward(self, x, max_length, trg):\n","        encoder_hidden, encoder_final = self.encode(x)\n","        return self.decode(encoder_hidden, encoder_final,max_length, trg)\n","    \n","    def encode(self, x):\n","        return self.encoder(x)\n","    \n","    def decode(self, encoder_hidden, encoder_final, max_length, trg, decoder_hidden=None):\n","        trg = trg.to(device)\n","        emb = self.embedding(trg)\n","        return self.decoder(emb, encoder_hidden, encoder_final,max_length, hidden=decoder_hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7LcsyhlP85f"},"source":["###Initializer"]},{"cell_type":"code","metadata":{"id":"AeP8Dytu5uuz"},"source":["def init_model(emb_size, vocab_size, encoder_final_size=128, hidden_size=256, \n","               num_layers=2, dropout=0.2, num_inp_channels=256, kernel_size=7, stride=3, pool_size=3, use_attention=True):\n","\n","    if use_attention is True:\n","        attention = AdditiveAttention(hidden_size)\n","        model = EncoderDecoder(\n","            Encoder_attention(num_inp_channels, hidden_size, encoder_final_size, kernel_size, stride, pool_size),\n","            Decoder_with_attention(attention, emb_size, encoder_final_size, hidden_size, vocab_size, num_layers=num_layers, dropout=dropout),\n","            nn.Embedding(vocab_size, emb_size),\n","            Generator(hidden_size, vocab_size))\n","        model = model.to(device)\n","    else:\n","        model = EncoderDecoder(\n","            Encoder_no_attention(num_inp_channels, hidden_size, encoder_final_size, kernel_size, stride, pool_size, dropout=dropout),\n","            Decoder_no_attention(emb_size, encoder_final_size, hidden_size, vocab_size, num_layers=num_layers, dropout=dropout),\n","            nn.Embedding(vocab_size, emb_size),\n","            Generator(hidden_size, vocab_size))\n","        model = model.to(device)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apZlXNmKS-Uv"},"source":["##Greedy decoding"]},{"cell_type":"code","metadata":{"id":"ybske-IdS9n7"},"source":["def greedy_decoding(sentence_vector, max_len, index_to_word, eos=None):\n","  \n","    output = np.array([])\n","    for i in range(max_len):\n","      index = torch.argmax(sentence_vector[i]).item()  # get the word index\n","      output = np.append(output, index_to_word[index]) # use Index to word dict to find out the corresponding word\n","    \n","    if eos is not None:\n","      first_eos = np.where(output==eos)[0]   # if the end of sentence token found, cut the word list keeping only the tokens before\n","      if len(first_eos) > 0:\n","        output = output[:first_eos[0]]\n","    \n","    sentence = \"\".join(output).capitalize() # for character level tokens\n","    \n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u9vpDLGr_Fiz"},"source":["## Loss"]},{"cell_type":"code","metadata":{"id":"izwYyHOrDtCG"},"source":["class SimpleLossCompute:\n","    def __init__(self, criterion, generator, opt=None):\n","        self.generator = generator\n","        self.criterion = criterion\n","        self.opt = opt\n","\n","    def __call__(self, x, y, norm):\n","        x = self.generator(x)\n","        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n","                              y.contiguous().view(-1))\n","        loss = loss / norm\n","\n","        if self.opt is not None:\n","            loss.backward()          \n","            self.opt.step()\n","            self.opt.zero_grad()\n","\n","        return loss.data.item() * norm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KoMn9pE8Ocxs"},"source":["## Training Loop"]},{"cell_type":"code","metadata":{"id":"Y3ENZVb4m7D1"},"source":["def addPadding(inputVector, size):\n","  if list(inputVector.shape)[2] < size:\n","    outputVector = torch.zeros(list(inputVector.shape)[0], list(inputVector.shape)[1], size) #Target\n","    outputVector[:, :, :inputVector.shape[2]] = inputVector\n","  else:\n","    outputVector = inputVector\n","  return outputVector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5oGTji3cObOd"},"source":["def run_epoch(path_and_text_data, batch_size, model, loss_compute, print_every=200):\n","    start = time.time()\n","    total_tokens = 0\n","    total_loss = 0\n","    print_tokens = 0\n","    path_and_text_data = shuffle(path_and_text_data)\n","    path_and_text_data = path_and_text_data.reset_index(drop = True)\n","    \n","    path_and_text_data = path_and_text_data[['ID', 'Description']]\n","    data_to_model_tpl  = [tuple(x) for x in path_and_text_data.to_numpy()]  \n","    data = chunked(data_to_model_tpl, batch_size)\n","\n","    for i, batch in enumerate(data):\n","      audios = [xx[0] for xx in batch]\n","      texts = [xx[1] for xx in batch]\n","      processed_audios = []\n","      batch_pase = None\n","      batch_texto = None #text for computing lost\n","      batch_texto_train = None #text for forward\n","      ntokens = 0\n","      audio_max = 0\n","      waveforms = []\n","      for z in audios:\n","        waveform, sample_rate = torchaudio.load(path + audio_path + z)      \n","        waveforms.append(waveform.T)\n","      waveform_batch = torch.nn.utils.rnn.pad_sequence(waveforms).to(device)\n","\n","      batch_pase = pase(waveform_batch.permute(1,2,0))\n","\n","      for u in texts:\n","        u = text_prep(u) # do the same pre-processing as in the extract vocabulary step \n","        tokens = list(u) # character based\n","        tokens.append(EOS_TOKEN)\n","        ntokens += len(tokens)\n","\n","        # create text for forward\n","        tokens_train = []\n","        for tkn in tokens:\n","          tokens_train.append(tkn) \n","        tokens_train.insert(0,SOS_TOKEN) # insert the sentence start token\n","        text_train = [word_to_index[k] for k in tokens_train]\n","        pad_train = [PAD_INDEX] * (max_len - len(text_train))\n","        text_train.extend(pad_train) \n","        text_train = torch.Tensor(text_train).type(torch.long) # text has size [max_len]\n","        text_train = text_train.view(1,text_train.size(0)) # change size to [1,max_len]\n","        text_train = text_train.to(device)\n","        batch_pase = batch_pase.to(device)\n","        if batch_texto_train is None:\n","          batch_texto_train = text_train\n","        else:\n","          batch_texto_train = torch.cat((batch_texto_train,text_train),0).to(device) # concatenate the target texts in the same batch\n","\n","        #create text for loss computing\n","        tokens_y = []\n","        for tkn in tokens:\n","          tokens_y.append(tkn) \n","        # no sentence start token here\n","        text_y = [word_to_index[k] for k in tokens_y]\n","        pad_y = [PAD_INDEX] * (max_len - len(text_y))\n","        text_y.extend(pad_y)\n","        text_y = torch.Tensor(text_y).type(torch.long).to(device) # text has size [max_len]\n","        text_y = text_y.view(1,text_y.size(0)) # change size to [1,max_len]\n","        if batch_texto is None:\n","          batch_texto = text_y\n","        else:\n","          batch_texto = torch.cat((batch_texto,text_y),0).to(device) # concatenate the target texts in the same batch\n","      \n","\n","      out, _ = model.forward(batch_pase, max_len, batch_texto_train)\n","\n","      loss = loss_compute(out, batch_texto, batch_size)\n","      total_loss += loss\n","      total_tokens += ntokens\n","      print_tokens += ntokens\n"," \n","      if model.training and i % print_every == 0:\n","        elapsed = time.time() - start\n","        print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f Time spent (sec): %f\" % (i, loss / batch_size, print_tokens / elapsed, elapsed))\n","        start = time.time()\n","        print_tokens = 0\n","\n","        trg = texts[0]\n","        print(\"trg: \", texts[0])\n","        print(\"pred:\", greedy_decoding(model.generator(out)[0], max_len, index_to_word, eos=EOS_TOKEN))\n","\n","    return math.exp(total_loss / float(total_tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XYtkQQDm22bd"},"source":["##Training"]},{"cell_type":"markdown","metadata":{"id":"QUFF_Dl42QPo"},"source":["### Define hyperparameters\n"]},{"cell_type":"code","metadata":{"id":"N6cvkqje2UCV"},"source":["embedding_size = 125\n","batch_size = 10\n","learning_rate = 0.001\n","num_epochs = 6\n","print_every = 200"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMSKrDjJ4_XY"},"source":["###Define train method\n"]},{"cell_type":"code","metadata":{"id":"TbPVmLCP24of"},"source":["def train(model, train_data, dev_data, num_epochs, lr, print_every, batch_size):    \n","\n","    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n","    optim = torch.optim.Adam(model.parameters(), lr=lr)    #optimizer\n","    dev_perplexities = []\n","\n","    for epoch in range(num_epochs):      \n","        print(\"Epoch\", epoch)\n","        model.train()\n","\n","        train_perplexity = run_epoch(train_data, batch_size, model,\n","                                     SimpleLossCompute(criterion, model.generator, optim),print_every=print_every)\n","        \n","        model.eval()\n","        \n","        with torch.no_grad():\n","\n","            dev_perplexity = run_epoch(dev_data, batch_size,\n","                                       model, \n","                                       SimpleLossCompute(criterion, model.generator, None), print_every=None)\n","            print(\"Validation perplexity: %f\" % dev_perplexity)\n","            dev_perplexities.append(dev_perplexity)\n","  \n","        \n","    return dev_perplexities"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_2wJ0Lt_sou"},"source":["###Define model"]},{"cell_type":"code","metadata":{"id":"nxHSWEIRl303","executionInfo":{"status":"ok","timestamp":1605618346431,"user_tz":-60,"elapsed":35475,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"8d358bc6-50a9-4a19-a901-8ed45ee4d5bd","colab":{"base_uri":"https://localhost:8080/"}},"source":["model_no_attention = init_model(emb_size=embedding_size, vocab_size=vocab_size, encoder_final_size=128, hidden_size=256, num_layers=1, \n","                                dropout=0.2, num_inp_channels=256, kernel_size=7, stride=3, pool_size=3, use_attention=False)\n","\n","#Get a global view of total trainable parameters\n","pytorch_total_params = 0\n","for p in model_no_attention.parameters():\n","  pytorch_total_params += p.numel()\n","  if p.requires_grad:\n","    print(p.name, p.data.shape)\n","print(\"Total variables: \" + str(pytorch_total_params))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None torch.Size([128, 256])\n","None torch.Size([128])\n","None torch.Size([768, 125])\n","None torch.Size([768, 256])\n","None torch.Size([768])\n","None torch.Size([768])\n","None torch.Size([256, 128])\n","None torch.Size([256])\n","None torch.Size([256, 381])\n","None torch.Size([40, 125])\n","None torch.Size([40, 256])\n","Total variables: 472840\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning:\n","\n","dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"25nBEFdRdggH","executionInfo":{"status":"ok","timestamp":1605618346432,"user_tz":-60,"elapsed":35472,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"eba121b7-79d5-4927-e974-5d3d0b5cb404","colab":{"base_uri":"https://localhost:8080/"}},"source":["model = init_model(emb_size=embedding_size, vocab_size=vocab_size, encoder_final_size = 128, hidden_size=256, num_layers=1, \n","                   dropout=0.2, num_inp_channels=256, kernel_size=7, stride=3, pool_size=3, use_attention=True)\n","#model = model.to(device)\n","#Get a global view of total trainable parameters\n","pytorch_total_params = 0\n","for p in model.parameters():\n","  pytorch_total_params += p.numel()\n","  if p.requires_grad:\n","    print(p.name, p.data.shape)\n","print(\"Total variables: \" + str(pytorch_total_params))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None torch.Size([256, 256, 7])\n","None torch.Size([256])\n","None torch.Size([128, 256])\n","None torch.Size([128])\n","None torch.Size([256, 256])\n","None torch.Size([256, 256])\n","None torch.Size([1, 256])\n","None torch.Size([768, 381])\n","None torch.Size([768, 256])\n","None torch.Size([768])\n","None torch.Size([768])\n","None torch.Size([256, 128])\n","None torch.Size([256])\n","None torch.Size([256, 637])\n","None torch.Size([40, 256])\n","None torch.Size([40])\n","None torch.Size([40, 125])\n","None torch.Size([40, 256])\n","Total variables: 1335600\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning:\n","\n","dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rdR86PPYULKo"},"source":["### Check drive\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"I6E3O4D6CmGs"},"source":["waveform, sample_rate = torchaudio.load(path+audio_path+'667626_18933d713e_0.wav')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-iLChenKRObj"},"source":["###Run this line to start training"]},{"cell_type":"code","metadata":{"id":"Kc8Kg3Uk3S5D"},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOo4EC2AC9tX"},"source":["torch.backends.cudnn.enabled = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuZ5-lWhmBVk"},"source":["dev_perplexities_no_attention = train(model_no_attention, train_data, dev_data, num_epochs=num_epochs,\n","                                      lr=learning_rate, print_every=print_every, batch_size = batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fimVTS0Y5hvV","executionInfo":{"status":"ok","timestamp":1605621549146,"user_tz":-60,"elapsed":3194607,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"30653903-bad1-4015-83a2-ba14583d4f64","colab":{"base_uri":"https://localhost:8080/"}},"source":["dev_perplexities = train(model, train_data, dev_data, num_epochs=num_epochs, lr=learning_rate, \n","                         print_every=print_every, batch_size = batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0\n","Epoch Step: 0 Loss: 189.602356 Tokens per Sec: 236.084090 Time spent (sec): 2.143304\n","trg:  A bird with a long orange bill drink water .\n","pred: <s>77y<s>ux7ymfw7<pad>77yos7<pad>a7oscei<s>72etujou1p7iyyi3fsisssssfst0sssessshseis7usgssqssssh007ss0sh7sseslssss0rsss0ssshs4h0sslsesss7pse<pad>vgssqsse8cgtsssss90spssgs0sss78s0t900ltfssssssssg74sss7sss\n","Epoch Step: 200 Loss: 73.656677 Tokens per Sec: 329.847121 Time spent (sec): 308.703619\n","trg:  A man in a gray shirt be win a race in the grass .\n","pred: A bon on a brei bkirt be ritta bick \n","Epoch Step: 400 Loss: 64.538170 Tokens per Sec: 314.451801 Time spent (sec): 323.496319\n","trg:  Basketball player push away opponent\n","pred: Aolketball player ilmt o  r \n","Validation perplexity: 2.954104\n","Epoch 1\n","Epoch Step: 0 Loss: 44.628353 Tokens per Sec: 609.482394 Time spent (sec): 0.682546\n","trg:  Person ride a green race motorcycle .\n","pred: Aerson ride a sraen sace oatorcycle \n","Epoch Step: 200 Loss: 52.120205 Tokens per Sec: 824.907972 Time spent (sec): 122.250000\n","trg:  A man in a green shirt on a half pipe , do a skateboard stunt .\n","pred: A man in a breen mhirt bn a sind aane owowa tkateboard \n","Epoch Step: 400 Loss: 42.052589 Tokens per Sec: 835.833687 Time spent (sec): 120.928364\n","trg:  A man in black and white be hold a ball with a red and blue stripe .\n","pred: A man bn alack and white si weld a wlll with h wed bnd wlue suaaped\n","Validation perplexity: 2.533021\n","Epoch 2\n","Epoch Step: 0 Loss: 61.040039 Tokens per Sec: 930.772747 Time spent (sec): 0.590907\n","trg:  A man be in an air upside-down in the snow .\n","pred: A man de pn t  itrbbn ide  w  an ahe anow \n","Epoch Step: 200 Loss: 46.208469 Tokens per Sec: 831.999866 Time spent (sec): 121.483193\n","trg:  This man be bungee jump .\n","pred: Awes man be peigee jump \n","Epoch Step: 400 Loss: 47.276909 Tokens per Sec: 829.789154 Time spent (sec): 123.327715\n","trg:  A group of man play soccer on the field .\n","pred: A proup of man olay foccer bn mhe sield \n","Validation perplexity: 2.330695\n","Epoch 3\n","Epoch Step: 0 Loss: 42.559860 Tokens per Sec: 708.340420 Time spent (sec): 0.571759\n","trg:  Two dog play in tall grass .\n","pred: Two dog play sn ahll grass \n","Epoch Step: 200 Loss: 37.150280 Tokens per Sec: 824.300139 Time spent (sec): 123.219681\n","trg:  Two trick biker , one be jump at the top of a ramp .\n","pred: Two teack ooker bone oe pump o  ahe cop of a ramp oarttoacetotccocettatcoootteco\n","Epoch Step: 400 Loss: 40.954418 Tokens per Sec: 810.721726 Time spent (sec): 123.681156\n","trg:  A horse race on a grass track\n","pred: A horse race on a rrassy\n","Validation perplexity: 2.172504\n","Epoch 4\n","Epoch Step: 0 Loss: 44.468052 Tokens per Sec: 738.014979 Time spent (sec): 0.764212\n","trg:  Marathon runner be race on a city street , with other person stand around .\n","pred: Tartreon rin er be ruce on a sity street \n","Epoch Step: 200 Loss: 33.741032 Tokens per Sec: 816.257771 Time spent (sec): 124.691003\n","trg:  A man in a yellow shirt surf .\n","pred: A man in a yellow shirt sknf \n","Epoch Step: 400 Loss: 36.917835 Tokens per Sec: 826.631113 Time spent (sec): 123.352483\n","trg:  Large dark bird over the water .\n","pred: Lorge jork berd fner ahe water \n","Validation perplexity: 2.065130\n","Epoch 5\n","Epoch Step: 0 Loss: 32.498051 Tokens per Sec: 732.488252 Time spent (sec): 0.629362\n","trg:  The Chocolate Lab jump too late to get a toy as the Black Lab capture it in a driveway .\n","pred: Fwicglattlayh wadywump ahw beyc ahwget t brw it hhe baack dadybat ire \n","Epoch Step: 200 Loss: 27.481104 Tokens per Sec: 831.942178 Time spent (sec): 122.257295\n","trg:  A teenage boy be grind a skateboard on a steep rail .\n","pred: A bennage boy de rrind a skateboard of a skaep reil \n","Epoch Step: 400 Loss: 37.188526 Tokens per Sec: 827.537155 Time spent (sec): 121.525661\n","trg:  Two person on a blue motorcycle .\n","pred: Two person bn a dlue aotorcycle \n","Validation perplexity: 2.027363\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BWpKMOJoIqGN"},"source":["## Plot Perplexity"]},{"cell_type":"code","metadata":{"id":"2KW91u4uIpHE","executionInfo":{"status":"ok","timestamp":1605621566169,"user_tz":-60,"elapsed":1835,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"4a3befec-3ab9-49a2-ee4e-62fd1e69c65b","colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["def plot_perplexity(perplexities):\n","    \"\"\"plot perplexities\"\"\"\n","    plt.title(\"Perplexity per Epoch\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Perplexity\")\n","    plt.plot(perplexities)\n","    \n","plot_perplexity(dev_perplexities)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"pORt20QJugq8"},"source":["##Save model"]},{"cell_type":"code","metadata":{"id":"xN9MqD1wujBZ"},"source":["#SAVE NO ATTENTION\n","#torch.save(model_no_attention.state_dict(), '/content/drive/My Drive/Project AI/models/model_no_attention_params_6k-8epochs.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfhb8pjUquzX"},"source":["#SAVE ATTENTION\n","torch.save(model.state_dict(), '/content/drive/My Drive/Project AI/models/model_attention_params-6k-6epochs_125emb_0001lrate.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"poCYnYcsXmC-"},"source":["##Load model"]},{"cell_type":"code","metadata":{"id":"_0oW6PzRXkuq"},"source":["#LOAD NO ATTENTION\n","model_no_attention.load_state_dict(torch.load('/content/drive/My Drive/Project AI/models/model_no_attention_params_6k-8epochs.pkl'))\n","model_no_attention = model_no_attention.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3FmGnHHqqpv"},"source":["#LOAD ATTENTION\n","model.load_state_dict(torch.load('/content/drive/My Drive/Project AI/models/model_attention_params-6k-6epochs_125emb_0001lrate.pkl'))\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GKIDYfCiYP3f"},"source":["##Use model to predict"]},{"cell_type":"code","metadata":{"id":"biqRrY6WmLcE"},"source":["predict_data = test_data[['ID','Description']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJLKs6jQULJO"},"source":["def model_pred_no_attention(model_no_attention, wav_file, max_len=100, sos_index=None, eos_index=None):\n","    with torch.no_grad():\n","        model_no_attention.eval()\n","        waveform, sample_rate = torchaudio.load(wav_file)       \n","        tensor_zeros = waveform.unsqueeze(1).to(device)\n","        pase_output = pase(tensor_zeros)\n","        encoder_hidden, encoder_final = model_no_attention.encode(pase_output)\n","        prev_y = torch.ones(1, 1).fill_(sos_index).type(torch.long)\n","        prev_y = prev_y.to(device)\n","        encoder_final = encoder_final.to(device)\n","        encoder_hidden = encoder_hidden.to(device)\n","        \n","\n","    output = []\n","    hidden = None\n","    #print(model_no_attention.embedding(prev_y).shape)\n","    for i in range(max_len):\n","        with torch.no_grad():\n","            pre_output, hidden = model_no_attention.decode(encoder_hidden, encoder_final, 1, prev_y, hidden)\n","            prob = model_no_attention.generator(pre_output[:, -1])\n","\n","        output.append(prob)\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.data.item()\n","        prev_y = torch.ones(1, 1).type(torch.long).fill_(next_word)\n","\n","    return (greedy_decoding(output, max_len, index_to_word, eos=EOS_TOKEN))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qjc-k5wbnYWA"},"source":["a=[]\n","results_original_no_att=pd.DataFrame()\n","#aux = predict_data\n","#aux = aux[0:20]\n","predict_data_list = predict_data['ID'].tolist()\n","for i in predict_data_list:\n","    res = model_pred_no_attention(model_no_attention, path + audio_path + i, max_len, word_to_index[SOS_TOKEN], word_to_index[EOS_TOKEN])\n","    original = predict_data[predict_data['ID']==i]\n","    original = original[['Description']]\n","    a.append(res)\n","    results_original_no_att = results_original_no_att.append(original)\n","\n","results_pred_no_att = pd.DataFrame(a)\n","\n","##Save to txt\n","results_pred_no_att.to_csv('/content/drive/My Drive/Project AI/models/results_no_att.txt', index = False, header=False)\n","results_original_no_att.to_csv('/content/drive/My Drive/Project AI/models/originals_no_att.txt', index = False, header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCIikwJPthAl","executionInfo":{"status":"ok","timestamp":1605544023534,"user_tz":-60,"elapsed":632,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"88ee5df2-c1e7-46e5-f3e3-62ada571f0e3","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["model_pred_no_attention(model_no_attention, path+audio_path+'166507476_9be5b9852a_0.wav', max_len, word_to_index[SOS_TOKEN], word_to_index[EOS_TOKEN])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'A man in a black dog be ride a skateboard '"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"DIg4-AgLYltR"},"source":["def model_pred(model, wav_file, max_len=100, sos_index=None, eos_index=None):\n","    with torch.no_grad():\n","        model.eval()\n","        waveform, sample_rate = torchaudio.load(wav_file)       \n","        tensor_zeros = waveform.unsqueeze(1).to(device)\n","        pase_output = pase(tensor_zeros)\n","        encoder_hidden, encoder_final = model.encode(pase_output)\n","        encoder_final =encoder_final.to(device)\n","        encoder_hidden = encoder_hidden.to(device)\n","        prev_y = torch.ones(1, 1).fill_(sos_index).type(torch.long)\n","        prev_y = prev_y.to(device)\n","\n","    output = []\n","    hidden = None\n","    ##print(model.embedding(prev_y).shape)\n","    for i in range(max_len):\n","        with torch.no_grad():\n","            pre_output, hidden = model.decode(encoder_hidden, encoder_final,1, prev_y, hidden)\n","            prob = model.generator(pre_output[:, -1])\n","\n","        output.append(prob)\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.data.item()\n","        prev_y = torch.ones(1, 1).type(torch.long).fill_(next_word)\n","\n","    return (greedy_decoding(output, max_len, index_to_word, eos=EOS_TOKEN))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJe9PiT2k9YU"},"source":["a=[]\n","results_original_att=pd.DataFrame()\n","predict_data_list = predict_data['ID'].tolist()\n","for i in predict_data_list:\n","    res = model_pred(model, path + audio_path + i, max_len, word_to_index[SOS_TOKEN], word_to_index[EOS_TOKEN])\n","    original = predict_data[predict_data['ID']==i]\n","    original = original[['Description']]\n","    a.append(res)\n","    results_original_att = results_original_att.append(original)\n","\n","results_pred_att = pd.DataFrame(a)\n","\n","##Save to txt\n","results_pred_att.to_csv('/content/drive/My Drive/Project AI/models/results_att.txt', index = False, header=False)\n","results_original_att.to_csv('/content/drive/My Drive/Project AI/models/originals_att.txt', index = False, header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJUkJFCUeYIw","executionInfo":{"status":"ok","timestamp":1605622058441,"user_tz":-60,"elapsed":1364,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"bce6c31d-6a10-4e5d-fc1a-f7a2e18d4ebb","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["model_pred(model,path+audio_path+'166507476_9be5b9852a_0.wav',max_len,word_to_index[SOS_TOKEN],word_to_index[EOS_TOKEN])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Fire boy be ride a water bottle with water '"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"5WTaFAE3asaD"},"source":["## WER"]},{"cell_type":"code","metadata":{"id":"FZt6XlyXICna","executionInfo":{"status":"ok","timestamp":1605622082865,"user_tz":-60,"elapsed":4503,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"01f67677-4fdc-4c2f-c8b9-692977ef1971","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install asr-evaluation"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting asr-evaluation\n","  Downloading https://files.pythonhosted.org/packages/36/23/3ab0b79dc4cec58412583bb9477ff37b655ec1f50390dc27400db95a7a14/asr_evaluation-2.0.4-py3-none-any.whl\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from asr-evaluation) (1.1.0)\n","Collecting edit-distance\n","  Downloading https://files.pythonhosted.org/packages/ab/ae/73a8cb612af25a0ed2292b402007a3288bdd43dd3ac67f2af29779ed337c/edit_distance-1.0.4-py3-none-any.whl\n","Installing collected packages: edit-distance, asr-evaluation\n","Successfully installed asr-evaluation-2.0.4 edit-distance-1.0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"otO4Gei4augz","executionInfo":{"status":"ok","timestamp":1605544141231,"user_tz":-60,"elapsed":902,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"894f5174-56e6-4fbd-bc41-d893d314c9a9","colab":{"base_uri":"https://localhost:8080/"}},"source":["#WER NO ATTENTION\n","!wer \"/content/drive/My Drive/Project AI/models/originals_no_att.txt\" \"/content/drive/My Drive/Project AI/models/results_no_att.txt\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence count: 500\n","WER:   112.523% (      6146 /       5462)\n","WRR:    17.063% (       932 /       5462)\n","SER:   100.000% (       500 /        500)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F4yJ36mGn-D8","executionInfo":{"status":"ok","timestamp":1605622086183,"user_tz":-60,"elapsed":1572,"user":{"displayName":"Sergio Chica","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEgqddE1tv8GHDTmuEHWcnEONOMCbNDzkvtfcGaw=s64","userId":"15054724198144968992"}},"outputId":"df4405ed-b720-46b6-bec4-0dca12896167","colab":{"base_uri":"https://localhost:8080/"}},"source":["#WER ATTENTION\n","!wer \"/content/drive/My Drive/Project AI/models/originals_att.txt\" \"/content/drive/My Drive/Project AI/models/results_att.txt\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence count: 500\n","WER:    71.531% (      3907 /       5462)\n","WRR:    33.614% (      1836 /       5462)\n","SER:    99.800% (       499 /        500)\n"],"name":"stdout"}]}]}